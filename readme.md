Μέρος Α: Προεπεξεργασία και Αναπαράσταση Κειμένου
1. Εισαγωγή

Στο παρόν μέρος πραγματοποιήθηκε η προεπεξεργασία του IMDB sentiment analysis dataset, με στόχο τη δημιουργία μίας ποιοτικής και αποδοτικής αναπαράστασης κειμένου που θα χρησιμοποιηθεί για εκπαίδευση κλασικών αλγορίθμων Μηχανικής Μάθησης.
Χρησιμοποιήθηκε το τοπικό IMDB dataset (pos/neg) και όχι η έκδοση του torchtext, ώστε να διασφαλιστεί ότι και οι δύο κλάσεις φορτώνονται σωστά.

2. Φόρτωση και Οργάνωση Δεδομένων

Η δομή του dataset είναι:

imdb/
   train/
      pos/
      neg/
   test/
      pos/
      neg/


Για τη φόρτωση χρησιμοποιήθηκε custom loader που:

διαβάζει όλα τα .txt αρχεία από pos και neg,

προσθέτει ετικέτες (0=neg, 1=pos),

τα ανακατεύει για να αποφευχθεί bias από τη σειρά των αρχείων.

Μετά τη φόρτωση επιβεβαιώθηκε ότι οι κλάσεις είναι ισορροπημένες:

TRAIN LABELS: [10000 10000]
DEV LABELS:   [2500 2500]

3. Tokenization

Το tokenization έγινε με κανονικές εκφράσεις (regex):

όλα τα γράμματα μετατρέπονται σε πεζά,

κρατούνται μόνο αλφαβητικοί χαρακτήρες,

δεν γίνεται αφαίρεση token λόγω μήκους (π.χ. "I", "bad", "am" κρατιούνται).

Αυτό διασφαλίζει ότι δεν θα χαθεί σημασιολογικό περιεχόμενο και ότι καμία κλάση δεν θα εξαφανιστεί λόγω υπερβολικού φιλτραρίσματος.

Παράδειγμα tokenization:

Κείμενο	Tokens
"I loved this movie! Amazing acting."	["i", "loved", "this", "movie", "amazing", "acting"]
4. Document Frequency (DF)

Για κάθε λέξη υπολογίστηκε το πλήθος των εγγράφων στα οποία εμφανίζεται (όχι το πλήθος εμφανίσεων).
Χρησιμοποιήθηκε Counter πάνω στα set(tokens) ώστε οι πολυεμφανίσεις μέσα στο ίδιο κείμενο να μην μετρούν.

Απομακρύνθηκαν λέξεις με DF < 5 ώστε να αποφευχθούν εξαιρετικά σπάνια tokens.

5. Αφαίρεση πιο συχνών και πιο σπάνιων tokens

Από το DF αφαιρέθηκαν:

τα 200 πιο συχνά tokens (n=200)

τα 5 πιο σπάνια (k=5)

Σκοπός:

τα πολύ συχνά tokens (π.χ. "the", "and", "is") δεν έχουν πληροφορία για συναίσθημα,

τα υπερβολικά σπάνια θορυβούν την IG.

Το αποτέλεσμα είναι ένα καθαρό σύνολο λέξεων πριν τον IG.

6. Υπολογισμός Information Gain (IG)

Για κάθε υποψήφιο token υπολογίστηκε:

η εντροπία H(Y),

η εντροπία παρουσίας H(Y|token),

η IG ως:

IG(token)=H(Y)−P(token)⋅H(Y∣token)−P(¬token)⋅H(Y∣¬token)


Υλοποιήθηκε βελτιστοποιημένη και γρήγορη IG, που υπολογίζει:

πόσα θετικά & αρνητικά κείμενα περιέχουν το token,

πόσα δεν το περιέχουν,

χωρίς διπλές επαναλήψεις ή αργά nested loops.

7. Επιλογή Λεξιλογίου (Vocabulary)

Τα tokens ταξινομήθηκαν βάσει Information Gain και επιλέχθηκαν:

top 3000 tokens (m=3000)

Τα tokens αντιστοιχήθηκαν σε δείκτες:

vocab[word] = index


Το παραγόμενο λεξιλόγιο αποθηκεύτηκε ως:

vocab.pkl

8. Μετατροπή Κειμένου σε Δυαδικό Διάνυσμα (Binary Bag-of-Words)

Για κάθε κείμενο:

δημιουργήθηκε πίνακας X[i, j] = 1 αν η λέξη vocab[j] υπάρχει στο κείμενο,

αλλιώς 0.

Η τελική αναπαράσταση είναι διαστάσεων:

Train:  20000 × 3000
Dev:     5000 × 3000


Αυτή η μορφή είναι κατάλληλη για Bernoulli Naive Bayes, Logistic Regression και Random Forests.

9. Outputs του Μέρους Α

Το ΜΕΡΟΣ Α παράγει:

✔ vocab.pkl (λεξιλόγιο)
✔ vectorized train/dev sets μέσω του κώδικα
✔ πλήρη διαδικασία tokenization → filtering → DF → IG → vocab → vectorization

Αυτή η έξοδος χρησιμοποιήθηκε επιτυχώς στο ΜΕΡΟΣ Β.

🔥 ΣΥΜΠΕΡΑΣΜΑ ΜΕΡΟΥΣ Α

Η διαδικασία προεπεξεργασίας ολοκληρώθηκε με επιτυχία και τα αποτελέσματα ήταν:

πλήρως ισορροπημένο dataset,

σωστή εξαγωγή Web–ready vocabulary,

ταχύτερος και ορθός υπολογισμός Information Gain,

σωστό binary Bag-of-Words representation,

 Αναφορά υπερ-παραμέτρων
n = 200

k = 5

m = 3000

RandomForest: n_estimators = 200

Logistic Regression: solver = liblinear, C = 1.0